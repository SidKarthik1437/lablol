
PROGRAM 1
Write a program to demonstrate Support Vector Machine using different Kernel functions.
CODE:
# Import the necessary libraries import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# Read the dataset
bankdata = pd.read_csv("bill_authentication_ML.csv")
# Get the shape bankdata.shape
# Display first 5 rows bankdata.head()
# Splitting into input and output variables X = bankdata.drop('Class', axis=1)
y = bankdata['Class']
# Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
# Model building
from sklearn.svm import SVC
svclassifier = SVC(kernel='linear')
svclassifier.fit(X_train, y_train)
y_pred = svclassifier.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
b) Non-linear SVM
# import necessary libraries import numpy as np
import matplotlib.pyplot as plt import pandas as pd
 a)
 Linear SVM
  # load the dataset
   Dept. of AI & DS, GAT ML II LAB - 20ADSL57

         url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
# Assign colum names to the dataset
colnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']
# Read dataset to pandas dataframe
irisdata = pd.read_csv(url, names=colnames)
X = irisdata.drop('Class', axis=1) y = irisdata['Class']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
(i) Polynomial Kernel
from sklearn.svm import SVC
svclassifier = SVC(kernel='poly', degree=8)
svclassifier.fit(X_train, y_train)
y_pred = svclassifier.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))
(ii) Gaussian Kernel (RBF)
from sklearn.svm import SVC
svclassifier = SVC(kernel='rbf')
svclassifier.fit(X_train, y_train)
y_pred = svclassifier.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred)) print(accuracy_score(y_test, y_pred))
(iii) Sigmoid Kernel
from sklearn.svm import SVC
svclassifier = SVC(kernel='sigmoid')
svclassifier.fit(X_train, y_train)
y_pred = svclassifier.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

PROGRAM 2
Write a program to implement Bagging and Boosting classifiers.
 CODE:
# Import the necessary libraries
# Numbers
import pandas as pd
# Plots
import seaborn as sns
import matplotlib.pyplot as plt
# Data processing, CSV file I/O (e.g. pd.read_csv)
# importing seaborn for statistical plots # matplotlib.pyplot plots data
# Train test split
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
#from sklearn import model_selection
# Sklearn package to evaluate a score by cross-validation
# Model
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
# Metrics
from sklearn import metrics
from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score from sklearn.metrics import precision_score from sklearn.metrics import recall_score from sklearn.metrics import f1_score
# Reading the data as a dataframe df = pd.read_csv('diabetes.csv')
# To model the Bagging classifier
# To model the Decision Tree classifier
# To model the AdaBoost classifier
# To model the Gradient Boosting classifier
# Checking the number of rows and columns in the data frame df.shape
# Displaying the first five records of the dataframe df.head()
# Information about the data df.info()
# Checking if there are any null values in the dataset df.isnull().values.any()
# Model building
X = df.drop('Outcome',axis=1) y = df['Outcome']
   Dept. of AI & DS, GAT ML II LAB - 20ADSL57

         # Splitting of data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) seed number
# Checking split of data
print('X training data size: {}'.format(X_train.shape))
print('y training data size: {}'.format(y_train.shape))
print('X testing data size: {}'.format(X_test.shape))
print('y testing data size: {}'.format(y_test.shape))
print("{0:0.2f}% of data is in training set".format((len(X_train)/len(df.index)) * 100)) print("{0:0.2f}% of data is in test set".format((len(X_test)/len(df.index)) * 100))
a) Bagging classifier
# 1 is just any random
 # Model fitting using X_train, X_test, y_train, y_test
dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)
bgcl = BaggingClassifier(base_estimator=dTree, n_estimators=50, random_state=1, max_samples=.7) bgcl = bgcl.fit(X_train, y_train)
# Predicting for test set bgcl_y_predict = bgcl.predict(X_test)
# Misclassified samples
misclassified_count = (y_test != bgcl_y_predict).sum() print('\nMisclassified samples in Bagging: ', misclassified_count)
# Confusion matrix
cm=metrics.confusion_matrix(y_test, bgcl_y_predict, labels=[0, 1]) df_cm = pd.DataFrame(cm, index = [i for i in ["Actual 0","Actual 1"]],
columns = [i for i in ["Predict 0","Predict 1"]]) plt.title('\nBagging Classifier Confusion Matrix') sns.heatmap(df_cm, annot=True, fmt='g');
# Performance metrics
a = accuracy_score(y_test, bgcl_y_predict) p = precision_score(y_test, bgcl_y_predict) r = recall_score(y_test, bgcl_y_predict)
f = f1_score(y_test, bgcl_y_predict) print("Accuracy : ",round(a,2)) print("Precision : ",round(p,2)) print("Recall : ",round(r,2))
print("F1 score : ",round(f,2))
b) Adaboost classifier
# Model fitting using X_train, X_test, y_train, y_test
abcl = AdaBoostClassifier(n_estimators=10, random_state=1) abcl = abcl.fit(X_train, y_train)
    Dept. of AI & DS, GAT ML II LAB - 20ADSL57

         # Predicting for test set abcl_y_predict = abcl.predict(X_test)
# Misclassified samples
misclassified_count = (y_test != abcl_y_predict).sum() print('\nMisclassified samples in Adaboost: ', misclassified_count)
# Confusion matrix
cm=metrics.confusion_matrix(y_test, abcl_y_predict, labels=[0, 1]) df_cm = pd.DataFrame(cm, index = [i for i in ["Actual 0","Actual 1"]],
columns = [i for i in ["Predict 0","Predict 1"]]) plt.title('\nAdaboost Classifier Confusion Matrix') sns.heatmap(df_cm, annot=True, fmt='g');
# Performance metrics
a = accuracy_score(y_test, abcl_y_predict) p = precision_score(y_test, abcl_y_predict) r = recall_score(y_test, abcl_y_predict)
f = f1_score(y_test, abcl_y_predict) print("Accuracy : ",round(a,2)) print("Precision : ",round(p,2)) print("Recall : ",round(r,2))
print("F1 score : ",round(f,2))
c) Gradient Boosting classifier
# Model fitting using X_train, X_test, y_train, y_test
gbcl = GradientBoostingClassifier(n_estimators = 50, random_state=1,learning_rate = 0.7) gbcl = gbcl.fit(X_train, y_train)
# Predicting for test set gbcl_y_predict = gbcl.predict(X_test)
# Misclassified samples
misclassified_count = (y_test != gbcl_y_predict).sum() print('\nMisclassified samples in Gradient Boost: ', misclassified_count)
# Confusion matrix
cm=metrics.confusion_matrix(y_test, gbcl_y_predict, labels=[0, 1]) df_cm = pd.DataFrame(cm, index = [i for i in ["Actual 0","Actual 1"]],
columns = [i for i in ["Predict 0","Predict 1"]]) plt.title('\nGradient Boost Classifier Confusion Matrix') sns.heatmap(df_cm, annot=True, fmt='g');
# Performance metrics
a = accuracy_score(y_test, gbcl_y_predict) p = precision_score(y_test, gbcl_y_predict) r = recall_score(y_test, gbcl_y_predict)
f = f1_score(y_test, gbcl_y_predict) print("Accuracy : ",round(a,2))
    Dept. of AI & DS, GAT ML II LAB - 20ADSL57

         print("Precision : ",round(p,2)) print("Recall : ",round(r,2)) print("F1 score : ",round(f,2))

PROGRAM 3
Write a program to demonstrate pipeline in Machine Learning.
CODE:
# Import the necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import make_pipeline
#Reading the data as a dataframe
dataset =pd.read_csv('vehicle_pgm_4.csv')
# Checking the number of columns and rows in data frame dataset.shape
# Displaying the first five records of the dataframe dataset.head()
# Information about the data dataset.info()
# Number of missing values in each column dataset.isnull().sum()

# Drop the missing values dataset.dropna()
# Model building
X = dataset.drop('class', axis=1) y = dataset['class']
# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
# Checking split of data
print('X training data size:', X_train.shape)
print('y training data size:', y_train.shape)
print('X testing data size:', X_test.shape)
print('y testing data size:', y_test.shape)
print("Data in training set:", (len(X_train)/len(dataset.index)) * 100) print("Data in test set:", (len(X_test)/len(dataset.index)) * 100)
# Create a pipeline and fit the model pipe_lr = make_pipeline(StandardScaler(),
PCA(n_components=8),
RandomForestClassifier(criterion='gini', n_estimators=20, max_depth=5))
    Dept. of AI & DS, GAT ML II LAB - 20ADSL57

         pipe_lr.fit(X_train, y_train)
y_pred = pipe_lr.predict(X_test) test_acc = pipe_lr.score(X_test, y_test) print(f'Test accuracy: {test_acc:.3f}')

PROGRAM 4
Write a program to classify the data using Multiclass classification algorithm 1.
CODE:
# Import the necessary libraries
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split from sklearn import svm, datasets
import matplotlib.pyplot as plt
import numpy as np
iris = datasets.load_iris()
#Store variables as target y and the first two features as X (sepal length and sepal width of the iris flowers) X = iris.data[:, :2]
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 0)
linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)
rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train, y_train) poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(X_train, y_train) sig = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train, y_train)
h = .01
#create the mesh
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h)) # create the title that will be shown on the plot
titles = ['Linear kernel','RBF kernel','Polynomial kernel','Sigmoid kernel']
for i, clf in enumerate((linear, rbf, poly, sig)):
#defines how many plots: 2 rows, 2columns=> leading to 4 plots plt.subplot(2, 2, i + 1) #i+1 is the index
#space between plots
plt.subplots_adjust(wspace=0.4, hspace=0.4)
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.PuBuGn, alpha=0.7)
 # Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.PuBuGn, plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())
plt.title(titles[i])
plt.show()
edgecolors='grey')
   Dept. of AI & DS, GAT ML II LAB - 20ADSL57

         linear_pred = linear.predict(X_test)
poly_pred = poly.predict(X_test)
rbf_pred = rbf.predict(X_test)
sig_pred = sig.predict(X_test)
accuracy_lin = linear.score(X_test, y_test) accuracy_poly = poly.score(X_test, y_test) accuracy_rbf = rbf.score(X_test, y_test) accuracy_sig = sig.score(X_test, y_test) print('Accuracy Linear Kernel:', accuracy_lin) print('Accuracy Polynomial Kernel:', accuracy_poly) print('Accuracy Radial Basis Kernel:', accuracy_rbf) print('Accuracy Sigmoid Kernel:', accuracy_sig)

PROGRAM 5
Write a program to classify the data using Multiclass classification algorithm 2.
CODE:
# Installing XGBoost pip install xgboost
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.metrics import confusion_matrix,classification_report, roc_auc_score,matthews_corrcoef,precision_score, recall_score, f1_score, accuracy_score import xgboost as xgb
# Reading the dataset
df = pd.read_csv('drug200.csv')
# Displaying first 5 rows df.head()
# Checking for null values df.isna().sum()
# Converting labels into numeric form labelencoder = LabelEncoder()
def label_encode(cat_feature):
return labelencoder.fit_transform(df[cat_feature]) cat_features = ['Sex','BP','Cholesterol','Drug']
for feature in cat_features:
df[feature+'_cat'] = label_encode(feature)
# Displaying first 5 rows after label encoding df.head()
# Splitting data
X,y = df[['Age','Sex_cat','BP_cat','Cholesterol_cat','Na_to_K']],df['Drug_cat'] X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y,random_state=42)
# Model building
xgb_classifier = xgb.XGBClassifier() xgb_classifier.fit(X_train,y_train)
prediction = xgb_classifier.predict(X_test) print(classification_report(prediction,y_test))
# Performance metrics print("Precision:{}".format(precision_score(prediction,y_test,average='weighted'))) print("Recall:{}".format(recall_score(prediction,y_test,average='weighted'))) print("F1 Score:{}".format((f1_score(prediction,y_test,average='weighted'))))

PROGRAM 6
Write a program to cluster the data using K-Means clustering algorithm.
CODE:
# Import the necessary libraries import pandas as pd
import matplotlib.pyplot as plt import seaborn as sns
# Reading the data as a dataframe
dataset = pd.read_csv('Mall_Customers (1).csv')
# Display first 5 rows dataset.head()
# Information about the data dataset.info()
# 5-point summary dataset.describe()
# using only Spending_Score and income variable for easy visualisation X = dataset.iloc[:, [2, 3]].values
# Using the elbow method to find the optimal number of clusters from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42) kmeans.fit(X)
# inertia method returns wcss for that model wcss.append(kmeans.inertia_)
# Plotting
plt.figure(figsize=(10,5))
sns.lineplot(range(1, 11), wcss,marker='o',color='red') plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()
# Fitting K-Means to the dataset
kmeans = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42) y_kmeans = kmeans.fit_predict(X)
# Visualising the clusters
plt.figure(figsize=(15,7))
sns.scatterplot(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], color = 'yellow', label = 'Cluster 1',s=50) sns.scatterplot(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], color = 'blue', label = 'Cluster 2',s=50) sns.scatterplot(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], color = 'green', label = 'Cluster 3',s=50) sns.scatterplot(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], color = 'grey', label = 'Cluster 4',s=50) sns.scatterplot(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], color = 'orange', label = 'Cluster 5',s=50) sns.scatterplot(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color = 'red',
    Dept. of AI & DS, GAT ML II LAB - 20ADSL57

         label = 'Centroids',s=150,marker='o') plt.grid(False)
plt.title('Clusters of customers') plt.xlabel('Annual Income (k$)') plt.ylabel('Spending Score (1-100)') plt.legend()
plt.show()

PROGRAM 7
Write a program to implement Label Propagation algorithm (Semi â€“ Supervised Learning).
CODE:
# Evaluate logistic regression fit on label propagation for semi-supervised learning from numpy import concatenate
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.semi_supervised import LabelPropagation from sklearn.linear_model import LogisticRegression
# define dataset
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1)
# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y)
# split train into labeled and unlabeled
X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train)
# create the training dataset input
X_train_mixed = concatenate((X_train_lab, X_test_unlab)) # create "no label" for unlabeled data
nolabel = [-1 for _ in range(len(y_test_unlab))]
# recombine training dataset labels
y_train_mixed = concatenate((y_train_lab, nolabel))
# define model
model = LabelPropagation()
# fit model on training dataset
model.fit(X_train_mixed, y_train_mixed)
# get labels for entire training dataset data
tran_labels = model.transduction_
# define supervised learning model
model2 = LogisticRegression()
# fit supervised learning model on entire training dataset model2.fit(X_train_mixed, tran_labels)
# make predictions on hold out test set
yhat = model2.predict(X_test)
# calculate score for test set
score = accuracy_score(y_test, yhat)
# summarize score
    Dept. of AI & DS, GAT ML II LAB - 20ADSL57

         print('Accuracy: %.3f' % (score*100))

PROGRAM 8
Write a program to demonstrate Random Forest algorithm and improve the performance using different Hyper Parameter Tuning Techniques (Randomized and Grid search CV).
CODE:
# Import the necessary libraries
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV from scipy.stats import randint
# Reading the data as a dataframe
df = pd.read_csv('winequality-red.csv')
# Checking the number of rows and columns in the data frame df.shape
# Displaying the first five records of the dataframe df.head()
# Information about the data df.info()
# Checking if there are any null values in the dataset df.isnull().values.any()
# Pre-processing df['target']=np.where(df['quality']>5,1,0) df2=df.drop(['quality'], axis=1) X=df2.drop(['target'], axis=1) y=df2['target']
# Initializing Random Forest rf=RandomForestClassifier()
a)Grid Search CV
# Initializing Random Forest rf=RandomForestClassifier()
# Grid Search CV
grid_space = {'max_depth' : [3,5,10,None],
'n_estimators': [10,100,200], 'max_features': [1,3,5,7], 'min_samples_leaf':[1,2,3], 'min_samples_split':[1,2,3]}
    Dept. of AI & DS, GAT ML II LAB - 20ADSL57

         grid = GridSearchCV(estimator=rf, param_grid=grid_space, cv= 3, scoring='accuracy') model_grid=grid.fit(X,y)
# Grid search results
print('Best grid search hyperparameters are:'+str(model_grid.best_params_)) print('Best grid search score is:'+str(model_grid.best_score_))
b)Randomized Search CV
# Initializing Random Forest rf=RandomForestClassifier()
# Random Search CV
rs_space = {'max_depth' : list(np.arange(10,100,10)) + [None],
'n_estimators': np.arange(10,500,50), 'max_features': randint(1,7),
'criterion' :['gini', 'entropy'], 'min_samples_leaf':randint(1,7), 'min_samples_split':np.arange(2,10,2)}
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = rs_space, n_iter = 100, cv = 3, scoring='accuracy')
model_random=rf_random.fit(X,y)
# Random search results
print('Best random search hyperparameters are:'+str(model_random.best_params_)) print('Best random search score is:'+str(model_random.best_score_))

Program 9
 BAYESIAN AND OPTUNA
import warnings
warnings.filterwarnings('ignore')
from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
df=pd.read_csv('/content/drive/MyDrive/All-Hyperparamter-Optimization-master/All-Hyperparamter-Optimization-master/diabetes.csv')
df.head()
 
import numpy as np
df['Glucose']=np.where(df['Glucose']==0,df['Glucose'].median(),df['Glucose'])
df['Insulin']=np.where(df['Insulin']==0,df['Insulin'].median(),df['Insulin'])
df['SkinThickness']=np.where(df['SkinThickness']==0,df['SkinThickness'].median(),df['SkinThickness'])
df.head()
 
X=df.drop('Outcome',axis=1)
y=df['Outcome']
pd.DataFrame(X,columns=df.columns[:-1])
 
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=0)
from sklearn.ensemble import RandomForestClassifier
rf_classifier=RandomForestClassifier(n_estimators=10).fit(X_train,y_train)
prediction=rf_classifier.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,prediction))
print(accuracy_score(y_test,prediction))
print(classification_report(y_test,prediction))
 
# Bayesian optimization
from hyperopt import hp,fmin,tpe,STATUS_OK,Trials
space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),
       'max_depth': hp.quniform('max_depth', 10, 1200, 10),
       'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),
       'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),
       'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),
       'n_estimators' : hp.choice('n_estimators', [10, 50, 300, 750, 1200,1300,1500])
   }
 
def objective(space):
   model = RandomForestClassifier(criterion = space['criterion'],
                                max_depth = space['max_depth'],
                                max_features = space['max_features'],
                                min_samples_leaf = space['min_samples_leaf'],
                                min_samples_split = space['min_samples_split'],
                                n_estimators = space['n_estimators'],
                                )
   
   accuracy = cross_val_score(model, X_train, y_train, cv = 5).mean()
 
   # We aim to maximize accuracy, therefore we return it as a negative value
   return {'loss': -accuracy, 'status': STATUS_OK }
from sklearn.model_selection import cross_val_score
trials = Trials()
best = fmin(fn= objective,
           space= space,
           algo= tpe.suggest,
           max_evals = 80,
           trials= trials)
best
crit = {0: 'entropy', 1: 'gini'}
feat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}
est = {0: 10, 1: 50, 2: 300, 3: 750, 4: 1200,5:1300,6:1500}
 
 
print(crit[best['criterion']])
print(feat[best['max_features']])
print(est[best['n_estimators']])
trainedforest = RandomForestClassifier(criterion = crit[best['criterion']], max_depth = best['max_depth'],
                                      max_features = feat[best['max_features']],
                                      min_samples_leaf = best['min_samples_leaf'],
                                      min_samples_split = best['min_samples_split'],
                                      n_estimators = est[best['n_estimators']]).fit(X_train,y_train)
predictionforest = trainedforest.predict(X_test)
print(confusion_matrix(y_test,predictionforest))
print(accuracy_score(y_test,predictionforest))
print(classification_report(y_test,predictionforest))
acc5 = accuracy_score(y_test,predictionforest)
 
!pip install optuna
import optuna
 
import sklearn.datasets
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
 
 
def objective(trial):
   iris = sklearn.datasets.load_iris()
   x, y = iris.data, iris.target
 
   criterion = trial.suggest_categorical("criterion", ["gini", "entropy"])
   max_depth = trial.suggest_int("max_depth", 2, 32, log=True)
   n_estimators = trial.suggest_int("n_estimators", 100,500)
 
   rf = sklearn.ensemble.RandomForestClassifier(criterion =criterion,
           max_depth=max_depth,
           n_estimators=n_estimators
       )
 
   score = cross_val_score(rf, x, y, n_jobs=-1, cv=3)
   accuracy = score.mean()
   return accuracy
 
 
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=15)
study.best_params
 

Program 10
 Q learning:
import numpy as op
# Define the environment
env np. array([1,1, 1, 1, 0, 1],
[-1, 1, 1, 0, 1, 100]
[-1, -1, 1, 0, 1, 11]
[-1, 0, 0, 1, 0,
1].
[e, 1, 1, 0,
1, 100],
[-1, 0, -1, -1, 0, 100]
])
# Define the 0 matrix
Q - np.zeros like (env)
# Define hyperparameters
alpha - 0.8
gamma = 0.95
num_episodes - 5000
# Perform Q-learning algorithm
for episode in range(num_episodes) :
state - np.random. randint(8, len (env)) while state Is 5:
action - np.random.choice(np.where(env[state] (- -1) [8])
next state = action
reward - env[state, action]
Q[state, action] - Q[state, action] + alpha * (reward + gamma " np.max (Q[next_state]) - [state, action]) state - next state
# Print the learned 0 matrix
print(Q)






